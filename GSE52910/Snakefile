configfile: "config.yaml"

################################################################################
### python modules
################################################################################

import os
import sys
import pandas as pd

################################################################################
### Custom functions
################################################################################

def get_samples():
    design_table = pd.read_csv(config["samples"], sep="\t", header=0)
    return list(design_table["sample"])

    # return ["dpf_3_delta_38_Hrs_Rev6_REPLICATE_B"]

def get_fq_1(wildcards):
    design_table = pd.read_csv(config["samples"], sep="\t", index_col="sample")
    return str(design_table.loc[wildcards.sample]["fq1"])

def get_fq_2(wildcards):
    design_table = pd.read_csv(config["samples"], sep="\t", index_col="sample")
    return str(design_table.loc[wildcards.sample]["fq2"])

################################################################################
### Finish
################################################################################

rule finish:
    input:
        salmon_filtered_all = os.path.join(config["output_dir"], "salmon", "quant_reads", "quant.genes.filtered.tsv"),
        salmon_TPM_genes_all = os.path.join(config["output_dir"], "salmon", "quant_reads", "quant.genes.TPM.tsv")
#        multiqc_dir = os.path.join(config["output_dir"], "summary", "qc"),
#        DE_salmon_quant_reads_with_transposon_annotation = expand(os.path.join(config["output_dir"], "DE__{experiment1}__{experiment2}", "DE_edgeR", "salmon_quant_reads", "final_table_with_transposons.tsv"),
#                                                                  zip,
#                                                                  experiment1 = [i[0]  for i in list(config["combinations"])],
#                                                                  experiment2 = [i[1]  for i in list(config["combinations"])]),
#        DE_salmon_quant_reads_transposons = expand(os.path.join(config["output_dir"], "DE__{experiment1}__{experiment2}", "DE_edgeR", "salmon_quant_reads_transposons", "final_table_FDR_low.tsv"),
#                                                                zip,
#                                                                experiment1 = [i[0]  for i in list(config["combinations"])],
#                                                                experiment2 = [i[1]  for i in list(config["combinations"])]),
#        DE_salmon_quant_reads_transposons_repName = expand(os.path.join(config["output_dir"], "DE__{experiment1}__{experiment2}", "DE_edgeR", "salmon_quant_reads_transposons_repName", "final_table_FDR_low.tsv"),
#                                                                        zip,
#                                                                        experiment1 = [i[0]  for i in list(config["combinations"])],
#                                                                        experiment2 = [i[1]  for i in list(config["combinations"])]),
#        DE_salmon_quant_reads_transposons_repFamily = expand(os.path.join(config["output_dir"], "DE__{experiment1}__{experiment2}", "DE_edgeR", "salmon_quant_reads_transposons_repFamily", "final_table_FDR_low.tsv"),
#                                                                          zip,
#                                                                          experiment1 = [i[0]  for i in list(config["combinations"])],
#                                                                          experiment2 = [i[1]  for i in list(config["combinations"])]),
#        DE_salmon_quant_reads_transposons_repClass = expand(os.path.join(config["output_dir"], "DE__{experiment1}__{experiment2}", "DE_edgeR", "salmon_quant_reads_transposons_repClass", "final_table_FDR_low.tsv"),
#                                                                         zip,
#                                                                         experiment1 = [i[0]  for i in list(config["combinations"])],
#                                                                         experiment2 = [i[1]  for i in list(config["combinations"])]),
#        htseq_counts_table = expand(os.path.join(config["output_dir"], "{sample}", "htseq", "count_genomic_alignment_htseq_unique_mappers.tsv"), sample=get_samples())
#        
################################################################################
################################################################################
################################################################################
################################################################################
################################################################################
### Quality statistics (Part 1)
################################################################################
################################################################################
################################################################################
################################################################################
################################################################################

################################################################################
### Fastqc
################################################################################

rule fastqc:
    input:
        fq_1 = lambda wildcards: get_fq_1(wildcards)
    output:
        outdir = directory(os.path.join(config["output_dir"], "{sample}", "fastqc"))
    conda:
        "envs/fastqc.yaml"
    log:
        os.path.join(config["local_log"], "fastqc_{sample}.log")
    shell:
        "(mkdir -p {output.outdir}; \
        fastqc \
        --outdir {output.outdir} \
        {input.fq_1}) &> {log}"

#################################################################################
#### Trim 3p adapter
#################################################################################
#
#rule trim_3p_adapter_SE:
#    input:
#        reads = lambda wildcards: get_fq_1(wildcards),
#    output:
#        reads = os.path.join(config["output_dir"], "{sample}", "cutadapt", "trim_3p_adapter.fastq.gz"),
#    params:
#        adapter = "AGATCGGAAGAGCACACGTCTGAAC",
#        error_rate = 0.1,
#        minimum_length = 15,
#        overlap = 3,
#    log:
#        os.path.join(config["local_log"], "trim_3p_adapter_SE_{sample}.log")
#    threads:    6
#    conda:  "envs/cutadapt.yaml"
#    shell:
#        "(cutadapt \
#        --adapter {params.adapter} \
#        --error-rate {params.error_rate} \
#        --minimum-length {params.minimum_length} \
#        --overlap {params.overlap} \
#        --cores {threads} \
#        {input.reads} | gzip > {output.reads}) &> {log}"
#
#################################################################################
### Index genome STAR
#################################################################################

rule index_genome_STAR:
    input:
        genome = config["genome"],
        annotation = config["gtf"]
    output:
        output = directory(os.path.join(config["output_dir"], "STAR_index"))
    params:
        outputdir = os.path.join(config["output_dir"],"STAR_index"),
        sjdbOverhang = config["sjdbOverhang"]
    threads:    40
    conda:
        "envs/STAR.yaml"
    log:
        os.path.join(config["local_log"], "index_genome_STAR.log")
    shell:
        "mkdir -p {output.output}; \
        chmod -R 777 {output.output}; \
        (STAR --runMode genomeGenerate \
        --sjdbOverhang {params.sjdbOverhang} \
        --genomeDir {params.outputdir} \
        --genomeFastaFiles {input.genome} \
        --runThreadN {threads} \
        --sjdbGTFfile {input.annotation}) &> {log}"

#################################################################################
### Align reads STAR
#################################################################################

rule align_reads_STAR:
    input:
        index = os.path.join(config["output_dir"], "STAR_index"),
        gtf = config["gtf"],
        reads = os.path.join(config["output_dir"], "{sample}", "cutadapt", "trim_3p_adapter.fastq.gz"),
    output:
        bam = os.path.join(config["output_dir"], "{sample}", "STAR", "{sample}_Aligned.sortedByCoord.out.bam"),
    params:
        outFileNamePrefix = os.path.join(config["output_dir"], "{sample}", "STAR", "{sample}_"),
        outputdir = os.path.join(config["output_dir"], "{sample}", "STAR"),
        outFilterMultimapNmax = 100,
        winAnchorMultimapNmax = 200
    log:
        os.path.join(config["local_log"],"align_reads_STAR_{sample}.log")
    threads:    20
    conda:
        "envs/STAR.yaml"
    shell:
        "(mkdir -p {params.outputdir}; \
        STAR --runMode alignReads \
        --twopassMode Basic \
        --runThreadN {threads} \
        --genomeDir {input.index} \
        --sjdbGTFfile {input.gtf} \
        --readFilesIn {input.reads} \
        --readFilesCommand zcat \
        --outFileNamePrefix {params.outFileNamePrefix} \
        --outSAMtype BAM SortedByCoordinate \
        --outFilterMultimapNmax {params.outFilterMultimapNmax} \
        --winAnchorMultimapNmax {params.winAnchorMultimapNmax}) &> {log}"

################################################################################
### Index alignment file
################################################################################

rule samtools_index_genomic_alignment:
    input:
        bam = os.path.join(config["output_dir"], "{sample}", "STAR", "{sample}_Aligned.sortedByCoord.out.bam")
    output:
        bai = os.path.join(config["output_dir"], "{sample}", "STAR", "{sample}_Aligned.sortedByCoord.out.bam.bai")
    log:
        os.path.join(config["local_log"],"samtools_index_genomic_alignment_{sample}.log")
    threads:    1
    conda:
        "envs/samtools.yaml"
    shell:
        "(samtools index {input.bam} > {output.bai}) &> {log}"

################################################################################
### Generate alfa genome index
################################################################################

rule alfa_genome_index:
    input:
        gtf = config["gtf"],
        chr_len = config["genome_size"]
    output:
        alfa_index_stranded = os.path.join(config["output_dir"], "annotation", "alfa_genome_index.stranded.ALFA_index"),
        alfa_index_unstranded = os.path.join(config["output_dir"], "annotation", "alfa_genome_index.unstranded.ALFA_index")
    params:
        genome_index_basename = os.path.join(config["output_dir"], "annotation", "alfa_genome_index")
    log:
        os.path.join(config["local_log"], "alfa_genome_index.log")
    threads:    8
    conda:
        "envs/alfa.yaml"
    shell:
        "(alfa \
        -a {input.gtf} \
        -g {params.genome_index_basename} \
        --chr_len {input.chr_len} \
        -p {threads}) &> {log}"

################################################################################
### Determine alignment statistics
################################################################################

rule alfa_aligment_statistics:
    input:
        bam = expand(os.path.join(config["output_dir"], "{sample1}", "STAR", "{sample1}_Aligned.sortedByCoord.out.bam"), sample1=get_samples()),
        bai = expand(os.path.join(config["output_dir"], "{sample1}", "STAR", "{sample1}_Aligned.sortedByCoord.out.bam.bai"), sample1=get_samples()),
        alfa_index_stranded = os.path.join(config["output_dir"], "annotation", "alfa_genome_index.stranded.ALFA_index"),
    output:
        pdf = os.path.join(config["output_dir"], "summary", "alfa_aligment_statistics", "alfa_aligment_statistics.Biotypes.pdf")
    params:
        genome_index_basename = os.path.join(config["output_dir"], "annotation", "alfa_genome_index"),
        output_prefix = "alfa_aligment_statistics",
        output_dir = os.path.join(config["output_dir"], "summary", "alfa_aligment_statistics"),
        bam_and_sample_name = expand(str(os.path.join(config["output_dir"], "{sample1}", "STAR", "{sample1}_Aligned.sortedByCoord.out.bam ")) +str("{sample1}"), sample1=get_samples()),
        strandness = "forward"
    log:
        os.path.join(config["local_log"],"alfa_aligment_statistics.log")
    threads:    8
    conda:
        "envs/alfa.yaml"
    shell:
        "(mkdir -p {params.output_dir}; \
        alfa \
        -g {params.genome_index_basename} \
        --bam {params.bam_and_sample_name} \
        -d 2 \
        --keep_ambiguous \
        --processors {threads} \
        --strandness {params.strandness} \
        --pdf {params.output_prefix} \
        --temp_dir {params.output_dir} \
        -o {params.output_dir}; \
        ) &> {log}"

################################################################################
### Multiqc
################################################################################

rule multiqc:
    input:
        fastqc = expand(os.path.join(config["output_dir"], "{sample}", "fastqc"), sample=get_samples()),
        pdf = os.path.join(config["output_dir"], "summary", "alfa_aligment_statistics", "alfa_aligment_statistics.Biotypes.pdf")
    output:
        multiqc_dir = directory(os.path.join(config["output_dir"], "summary", "qc"))
    log:
        os.path.join(config["local_log"], "multiqc_fastqc.log")
    conda:
        "envs/multiqc.yaml"
    shell:
        "(multiqc --outdir {output.multiqc_dir} results logs) &> {log}"


################################################################################
################################################################################
################################################################################
################################################################################
################################################################################
### Create custom annotation
################################################################################
################################################################################
################################################################################
################################################################################
################################################################################

################################################################################
### Concatenate normal transcripts and transcripts annotated as
### transposons transcripts
### ALL (before filtering)
################################################################################

rule concatenate_canonical_and_transposon_transcripts_all:
    input:
        gtf = config["gtf"],
        gtf_transposon_transcripts = config["gtf_transposon_transcripts"]
    output:
        gtf = os.path.join(config["output_dir"], "annotation", "annotation.canonical_and_transposon_transcripts.gtf")
    log:
        os.path.join(config["local_log"], "concatenate_canonical_and_transposon_transcripts_all.log")
    shell:
        "(cat {input.gtf} {input.gtf_transposon_transcripts} > {output.gtf}) &> {log}"

################################################################################
### Filter small RNAs from annotation file
################################################################################

rule filter_RNAs_from_gtf:
    input:
        gtf = config["gtf"]
    output:
        gtf = os.path.join(config["output_dir"], "annotation", "annotation.filtered.gtf")
    log:
        os.path.join(config["local_log"], "filter_RNAs_from_gtf.log")
    shell:
        "(grep -P \"\texon\t\" {input.gtf} | \
        grep -P -v \"miRNA|piRNA|pre_miRNA|rRNA|rRNA_pseudogene|snRNA|snoRNA|tRNA|tRNA_pseudogene\" | \
        grep -P -v \"^MtDNA\" > {output.gtf}) &> {log}"

################################################################################
### Concatenate normal transcripts and transcripts annotated as
### transposons transcripts
################################################################################

rule concatenate_canonical_and_transposon_transcripts:
    input:
        gtf = os.path.join(config["output_dir"], "annotation", "annotation.filtered.gtf"),
        gtf_transposon_transcripts = config["gtf_transposon_transcripts"]
    output:
        gtf = os.path.join(config["output_dir"], "annotation", "annotation.filtered.canonical_and_transposon_transcripts.gtf")
    log:
        os.path.join(config["local_log"], "concatenate_canonical_and_transposon_transcripts.log")
    shell:
        "(cat \
        {input.gtf} \
        <(grep -P \"\texon\t\" {input.gtf_transposon_transcripts}) \
        > {output.gtf}) &> {log}"

################################################################################
### Extract canonical and transposon transcripts sequences
################################################################################

rule extract_canonical_and_transposon_transcripts_sequences:
    input:
        gtf = os.path.join(config["output_dir"], "annotation", "annotation.filtered.canonical_and_transposon_transcripts.gtf"),
        genome = config["genome"]
    output:
        seq = os.path.join(config["output_dir"], "annotation", "annotation.filtered.canonical_and_transposon_transcripts.fa")
    conda:
        "envs/cufflinks.yaml"
    log:
        os.path.join(config["local_log"], "extract_miRNA_sequences.log")
    shell:
        "(gffread \
        {input.gtf} \
        -g {input.genome} \
        -w {output.seq} \
        ) &> {log}"

################################################################################
### Index canonical and transposon transcripts with salmon
################################################################################

rule index_canonical_and_transposon_transcripts_salmon:
    input:
        transcripts = os.path.join(config["output_dir"], "annotation", "annotation.filtered.canonical_and_transposon_transcripts.fa")
    output:
        index = directory(os.path.join(config["output_dir"], "annotation", "annotation.filtered.canonical_and_transposon_transcripts.salmon.idx"))
    params:
        kmer = 21
    threads:    20
    conda:
        "envs/salmon.yaml"
    log:
        os.path.join(config["local_log"], "index_canonical_and_transposon_transcripts_salmon.log")
    shell:
        "(salmon index \
        -t {input.transcripts} \
        -i {output.index} \
        -k {params.kmer}) &> {log}"

################################################################################
### canonical and transposon transcripts transcript id to gene id
################################################################################

rule transcript_id_to_gene_id_canonical_and_transposon_transcripts:
    input:
        gtf = os.path.join(config["output_dir"], "annotation", "annotation.filtered.canonical_and_transposon_transcripts.gtf"),
    output:
        transcriptid2geneid = os.path.join(config["output_dir"], "annotation", "annotation.filtered.canonical_and_transposon_transcripts.transcriptid2geneid.txt")
    log:
        os.path.join(config["local_log"], "transcript_id_to_gene_id_canonical_and_transposon_transcripts.log")
    shell:
        "(cut -f 9 {input.gtf} | \
        cut -f 1-2 -d \";\" | \
        sed \'s/gene_id \"//\' | \
        sed \'s/\"; transcript_id \"/ /\' | \
        sed \'s/\\\"//\' | \
        sort -u | \
        awk \'{{print $2 \" \" $1}}\' \
        > {output.transcriptid2geneid} \
        ) &> {log}"
        
################################################################################
### ucsc repeats to gtf
################################################################################

rule ucsc_repeats_to_gtf:
    input:
        repeats =  config["repeats"],
        script = os.path.join(config["scripts"], "ucsc_repeats_to_gtf.py")
    output:
        gtf = os.path.join(config["output_dir"], "annotation", "ce_11_repeats.gtf"),
        bed = os.path.join(config["output_dir"], "annotation", "ce_11_repeats.bed"),
    log:
        os.path.join(config["local_log"], "ucsc_repeats_to_gtf.log")
    shell:
        "(python {input.script} \
        --ucsc_repeats {input.repeats} \
        --gtf {output.gtf} \
        --bed {output.bed} \
        --verbose) &> {log}"

################################################################################
### filter repeats gtf (also remove chr from beginning of the file)
################################################################################

rule filter_repeats_gtf:
    input:
        gtf = os.path.join(config["output_dir"], "annotation", "ce_11_repeats.gtf")
    output:
        gtf = os.path.join(config["output_dir"], "annotation", "ce_11_repeats.filtered.gtf")
    log:
        os.path.join(config["local_log"], "filter_repeats_gtf.log")
    shell:
        "(grep -P -v 'repClass \"Simple_repeat\"|repClass \"Low_complexity\"|repClass \"rRNA\"\' \
         {input.gtf} | \
         sed 's/^chr//' > {output.gtf}) &> {log}"
         
################################################################################
### filter repeats bed (also remove chr from beginning of the file)
################################################################################

rule filter_repeats_bed:
    input:
        bed = os.path.join(config["output_dir"], "annotation", "ce_11_repeats.bed"),
    output:
        bed = os.path.join(config["output_dir"], "annotation", "ce_11_repeats.filtered.bed"),
    log:
        os.path.join(config["local_log"], "filter_repeats_bed.log")
    shell:
        "(grep -P -v 'Simple_repeat|Low_complexity|rRNA' \
         {input.bed} | \
         sed 's/^chr//' > {output.bed}) &> {log}"
         
################################################################################
### Create different categories of transposon mappings
################################################################################

rule create_transposon_mappings:
    input:
        gtf = os.path.join(config["output_dir"], "annotation", "ce_11_repeats.filtered.gtf")
    output:
        transposon2repName = os.path.join(config["output_dir"], "annotation", "transposon2repName.tsv"),
        transposon2repClass = os.path.join(config["output_dir"], "annotation", "transposon2repClass.tsv"),
        transposon2repFamily = os.path.join(config["output_dir"], "annotation", "transposon2repFamily.tsv")
    log:
        os.path.join(config["local_log"], "create_transposon_mappings.log")
    shell:
        "(cut -f 9 {input.gtf} \
        | cut -f 1,2 -d \";\" \
        | sed 's/; repName \"/\\t/' \
        | sed 's/\"//' > {output.transposon2repName}; \
        cut -f 9 {input.gtf} \
        | cut -f 1,3 -d \";\" \
        | sed 's/; repClass \"/\\t/' \
        | sed 's/\"//' > {output.transposon2repClass}; \
        cut -f 9 {input.gtf} \
        | cut -f 1,4 -d \";\" \
        | sed 's/; repFamily \"/\\t/' \
        | sed 's/\"//' > {output.transposon2repFamily}; \
        ) &>{log}"

##################################################################################
### Extract transposon sequences
##################################################################################

rule extract_transposon_sequences:
    input:
        annotation = os.path.join(config["output_dir"], "annotation", "ce_11_repeats.filtered.gtf"),
        genome = config["genome"]
    output:
        transcripts = os.path.join(config["output_dir"], "annotation", "ce_11_repeats.filtered.fa")
    conda:
        "envs/cufflinks.yaml"
    log:
        os.path.join(config["local_log"],"extract_transposon_sequences.log")
    shell:
        "(gffread {input.annotation} \
        -g {input.genome} \
        -w {output.transcripts}) &> {log}"
        
################################################################################
### Index transposon with salmon
################################################################################

rule index_transposons_salmon:
    input:
        transcripts = os.path.join(config["output_dir"], "annotation", "ce_11_repeats.filtered.fa")
    output:
        index = directory(os.path.join(config["output_dir"], "annotation", "ce_11_repeats.filtered.salmon.idx"))
    params:
        kmer = 21
    threads:    20
    conda:
        "envs/salmon.yaml"
    log:
        os.path.join(config["local_log"], "index_transposons_salmon.log")
    shell:
        "(salmon index \
        -t {input.transcripts} \
        -i {output.index} \
        -k {params.kmer}) &> {log}"
        
        
# ################################################################################
# ### transposons to transposons family
# ################################################################################

# rule transposons_to_transposons_family:
#     input:
#         gff = config["gff_transposons"]
#     output:
#         transposons2transposons_family = os.path.join(config["output_dir"], "annotation", "annotation.transposons_to_transposons_family.txt")
#     log:
#         os.path.join(config["local_log"], "transposons_to_transposons_family.log")
#     shell:
#         "(cut -f 9 {input.gff} | \
#         grep Family | grep Name | \
#         cut -f 2,3 -d \";\" | \
#         sed \'s/Name=//\' | \
#         sed \'s/;Family=/ /\' | \
#         sort -u \
#         > {output.transposons2transposons_family} \
#         ) &> {log}"
        
        
# ################################################################################
# ### gff 2 bed for transposons (Transposon Elements)
# ################################################################################

# rule gff_transposons_2_bed:
#     input:
#         gff = config["gff_transposons"],
#         script = os.path.join(config["scripts"], "gff_transposons_2_bed.py")
#     output:
#         bed = os.path.join(config["output_dir"], "annotation", "c_elegans.PRJNA13758.WS270.annotations.WormBase_transposon.transposons.bed")
#     log:
#         os.path.join(config["local_log"], "gff_transposons_2_bed.log")
#     conda:
#         "envs/HTSeq_pandas_samtools_seaborn.yaml"        
#     shell:
#         "(python {input.script} \
#         --gff {input.gff} \
#         --bed {output.bed} \
#         --verbose \
#         ) &> {log}"

# ################################################################################
# ################################################################################
# ################################################################################
# ################################################################################
# ################################################################################
# ### Quantify and DE salmon quant reads
# ################################################################################
# ################################################################################
# ################################################################################
# ################################################################################
# ################################################################################

################################################################################
### Salmon quantify based on reads
################################################################################

rule salmon_quant_reads:
    input:
        reads = lambda wildcards: get_fq_1(wildcards),
        index = os.path.join(config["output_dir"], "annotation", "annotation.filtered.canonical_and_transposon_transcripts.salmon.idx"),
        gtf = os.path.join(config["output_dir"], "annotation", "annotation.filtered.canonical_and_transposon_transcripts.transcriptid2geneid.txt")
    output:
        salmon_out = os.path.join(config["output_dir"], "{sample}", "salmon", "quant_reads", "quant.sf"),
        salmon_genes_out = os.path.join(config["output_dir"], "{sample}", "salmon", "quant_reads", "quant.genes.sf"),
    params:
        libType = "A",
        salmon_dir = os.path.join(config["output_dir"], "{sample}", "salmon", "quant_reads")
    log:
        os.path.join(config["local_log"], "salmon_quant_reads_{sample}.log")
    threads:    12
    conda:
        "envs/salmon.yaml"
    shell:
        "(salmon quant \
        --libType {params.libType} \
        --seqBias \
        --validateMappings \
        --threads {threads} \
        --writeUnmappedNames \
        --index {input.index} \
        --geneMap {input.gtf} \
        --unmatedReads {input.reads} \
        -o {params.salmon_dir}) &> {log}"

################################################################################
### Filter salmon and round reads
################################################################################

rule filter_salmon_quant_reads:
    input:
        salmon_genes_out = os.path.join(config["output_dir"], "{sample}", "salmon", "quant_reads", "quant.genes.sf"),
    output:
        salmon_filtered = os.path.join(config["output_dir"], "{sample}", "salmon", "quant_reads", "quant.genes.filtered.tsv")
    log:
        os.path.join(config["local_log"], "filter_salmon_quant_reads_{sample}.log")
    run:
        df = pd.read_csv(input.salmon_genes_out, header=0, sep="\t")
        df = df[["Name", "NumReads"]].copy()
        df.columns = ["Name", "counts"]
        df["counts"] = df["counts"].round()
        df["counts"] = df["counts"].astype(int)
        df.set_index("Name", inplace=True)
        df.to_csv(output.salmon_filtered, header=True, sep="\t", index=True)

################################################################################
### Collect all salmon reads
################################################################################

rule collect_salmon_reads:
    input:
        salmon_filtered = expand(os.path.join(config["output_dir"], "{sample}", "salmon", "quant_reads", "quant.genes.filtered.tsv"), sample=get_samples())
    output:
        salmon_filtered_all = os.path.join(config["output_dir"], "salmon", "quant_reads", "quant.genes.filtered.tsv")
    params:
        sample_name = expand("{sample}", sample=get_samples())
    log:
        os.path.join(config["local_log"], "collect_salmon_reads.log")
    run:
        df_merge = pd.DataFrame()
        for i in range(len(input.salmon_filtered)): 
            df = pd.read_csv(input.salmon_filtered[i], header=0, sep="\t")
            df.columns = ["Name", params.sample_name[i]]
            if df_merge.empty:
                df_merge = df.copy()
            else:
                df_merge = pd.merge(df_merge, df, on="Name")
        df_merge.to_csv(output.salmon_filtered_all, header=True, sep="\t", index=False)

################################################################################
### Filter salmon (TPM)
################################################################################

rule filter_salmon_TPM:
    input:
        salmon_genes_out = os.path.join(config["output_dir"], "{sample}", "salmon", "quant_reads", "quant.genes.sf"),
    output:
        salmon_filtered = os.path.join(config["output_dir"], "{sample}", "salmon", "quant_reads", "quant.genes.TPM.tsv")
    log:
        os.path.join(config["local_log"], "filter_salmon_TPM_{sample}.log")
    run:
        df = pd.read_csv(input.salmon_genes_out, header=0, sep="\t")
        df = df[["Name", "TPM"]].copy()
        df.set_index("Name", inplace=True)
        df.to_csv(output.salmon_filtered, header=True, sep="\t", index=True)

################################################################################
### Collect all salmon reads TPM
################################################################################

rule collect_salmon_reads_TPM:
    input:
        salmon_filtered = expand(os.path.join(config["output_dir"], "{sample}", "salmon", "quant_reads", "quant.genes.TPM.tsv"), sample=get_samples())
    output:
        salmon_filtered_all = os.path.join(config["output_dir"], "salmon", "quant_reads", "quant.genes.TPM.tsv")
    params:
        sample_name = expand("{sample}", sample=get_samples())
    log:
        os.path.join(config["local_log"], "collect_salmon_reads_TPM.log")
    run:
        df_merge = pd.DataFrame()
        for i in range(len(input.salmon_filtered)): 
            df = pd.read_csv(input.salmon_filtered[i], header=0, sep="\t")
            df.columns = ["Name", params.sample_name[i]]
            if df_merge.empty:
                df_merge = df.copy()
            else:
                df_merge = pd.merge(df_merge, df, on="Name")
        df_merge.to_csv(output.salmon_filtered_all, header=True, sep="\t", index=False)

